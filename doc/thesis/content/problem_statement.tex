\chapter{Обзор литературы}

\section{Постановка задачи}
В вероятностном тематическом моделировании коллекция документов рассматривается как множество троек $(d, w, t)$, выбранных случайно и независимо из дискретного распределения $p(d, w, t)$, заданного на конечном множестве $D \times W \times T$. Документы $d \in  D$ и термины $w \in W$ являются наблюдаемыми переменными, 
тема $t \in T$ является латентной (скрытой) переменной. Построить тематическую модель коллекции документов $D$ — значит найти
распределения $p(w |t)$ для всех тем $t \in T$ и распределения $p(t| d)$  для всех документов $d \in  D$. Известными при это являются распределения $p(w|d)$ терминов 
(токенов) в документах коллекции.

Предполагается, что порядок токенов $w \in W$ в документе $d \in D$ не важен (гипотеза \lq\lq{мешка слов}\rq\rq{}), что позволяет представить коллекцию в виде матрицы $[n_{dw} ]_{D \times W}$, где $n_{dw}$ --- число вхождений $w$ в $d$. Также коллекцию можно представить в виде матрицы эмпирических оценок вероятности встретить токен в документе $[p_{dw}]_{D\times W}$, где $p_{dw}=\hat p(w|d) = \dfrac {n_{dw}}{n_d}$, $n_d = \sum\limits_{w \in W} n_{dw}$ --- число слов в документе $d$. Кроме того, используется гипотеза условной независимости: $p(w|t,d)=p(w|t)$.

\section{Плоские тематические модели}
При сделанных предположениях плоская (одноуровневая) тематическая модель описывается формулой 
\begin{align} 
p(w|d) \approx \sum\limits_{t \in T}  p(w|t)p(t|d) \ \ d\in D, w \in W, \tag{*}\label{bayes}
\end{align}
которая следует из определения условной вероятности и формулы полной вероятности.
Пусть число тем $|T|$ много меньше числа документов $|D|$ и числа терминов $|W|$. Тогда представим задачу (\ref{bayes}) в виде факторизации матрицы $F =[p(w|d)]_{W \times D} $: $$F \approx \Phi \Theta.$$
Параметры модели --- матрицы $\Phi = [\phi_{wt}]_{W \times T}, \ \phi_{wt}=p(w|t)$ (термины, относящиеся к темам) и $\Theta = [\theta_{td}]_{T \times D}, \ \theta_{td}=p(t|d)$ (темы документов) --- такие, что $$\sum\limits_{w \in W} \phi_{wt} = 1, \ \sum\limits_{t \in T} \theta_{td} = 1.$$ 

В ARTM матрицы $\Phi$ и $\Theta$ находятся с помощью максимизации  суммы логарифма правдоподобия и регуляризаторов $R_i$ с неотрицательными коэффициентами регуляризации
$\tau_i$ \cite{ARTM4}: 
$$ \sum\limits_{d \in D} \sum\limits_{w \in W} n_{dw} \ln{\sum\limits_{t \in T} \phi_{wt}\theta_{td}} + \sum_{i} \tau_i R_i(\Phi, \Theta) \rightarrow 
\max\limits_{\Phi, \Theta}$$ 
$$ \text{при условиях}  \ \phi_{wt} \geq 0, \ \theta_{td} \geq 0,\ \sum\limits_{w \in W} \phi_{wt} = 1, \ \sum\limits_{t \in T} \theta_{td} = 1.$$

\subsection{Мультимодальные тематические модели}
Документы $d \in D$ могут содержать не только текст, но и другие элементы, такие как тэги, ссылки, имена авторов и т.д. Такие типы элементов называются модальностями. Пусть $M$ --- множество модальностей. Каждой $m \in M$ соответсвует отдельный словарь (множество токенов)  $W^m$, причем $W = \bigsqcup\limits_{m \in M} W^m $.

Пусть $F^m=[p(w|d)]_{W^m \times D}$, $m \in M$ --- матрицы наблюдаемых вероятностей для каждой модальности, а $\Phi^m$ --- соответсвующие матрицы скрытых вероятностей $p(w|t)$. Определим $F$ и $\Phi$ как объединения строк $F^m$ и $\Phi^m, \ m \in M$ соответственно. Тогда получим задачу факторизации $F \approx \Phi \Theta$ для мультимодальной тематической модели.

При построении мультимодальной тематической модели максимизируется взвешенная сумма логарифмов правдоподобия для всех модальностей $m \in M$ и регуляризаторов $R_i$ \cite{ARTM4}: 
$$ \sum\limits_{m \in M} \kappa_m \sum\limits_{d \in D} \sum\limits_{w \in W^m} n_{dw} \ln{\sum\limits_{t \in T} \phi_{wt}\theta_{td}} + \sum_{i} \tau_i R_i(\Phi, \Theta) \rightarrow 
\max\limits_{\Phi, \Theta}$$
$$ \text{при условиях}  \ \phi_{wt} \geq 0, \ \theta_{td} \geq 0,\ \sum\limits_{w \in W} \phi_{wt} = 1, \ \sum\limits_{t \in T} \theta_{td} = 1.$$
Как показано в \cite{ARTM1}, эта оптимизационная задача решается EM-алгоритмом.

E-шаг: $$p(t|d, w) = p_{tdw} =\norm\limits_{t\in T}(\phi_{wt}\theta_{td}).$$

M-шаг: $$\phi_{wt} = \norm\limits_{w \in W}\Bigl(n_{wt}+\phi_{wt}\sum\limits_{i}\tau_i\dfrac{\partial  R_i}{\partial \phi_{wt}}\Bigr); \ \  \ \ n_{wt} = \sum\limits_{d \in D} n_{dw} p_{tdw};$$
	$$ \theta_{td} = \norm\limits_{t \in T}\Bigl(n_{td}+\theta_{td}\sum\limits_{i}\tau_i\dfrac{\partial R_i}{\partial \theta_{td}}\Bigr); \ \  \ \ n_{td} = \sum\limits_{w \in d} n_{dw} p_{tdw}.$$

\subsection{Регуляризаторы}
\textbf{Сглаживание и разреживание}:

Регуляризатор сглаживания вводит в модель требование, чтобы столбцы $\phi_{t}$ и $\theta_{d}$ были близки к заданным распределениям $\beta_t = [\beta_{wt}]_{w \in W}$ и $\alpha_d = [\alpha_{td}]_{t \in T}$ в смысле дивиргенции Кульбака-Лейблера\cite{ARTM4}:
$$R(\Phi, \Theta) = \beta_0\sum_{m\in M} \sum_{t\in T}\sum_{w \in W^m} \beta_{wt}\ln \phi_{wt} + \alpha_0 \sum_{d\in D} \sum_{t\in T}\alpha_{td} \ln \theta_{td} \rightarrow 
\max\limits_{\Phi, \Theta},$$
где $\beta_0$ и $\alpha_0$ --- заданные положительные коэффициенты.

Регуляризатор разреживания имеет такой же вид, но коэффициенты $\beta_0$ и $\alpha_0$ отрицательны. Он способствует обращению значительной части вероятностей $\phi_{wt}$ и $\theta_{td}$ в ноль, что соответствует ествественному предположению о том, что каждый документ $d$ и каждый токен $w$ связаны лишь с небольшим числом тем $t$.


\textbf{Декоррелирование:}

Регуляризатор декоррелирования минимизирует ковариации между столбцами $\phi_{t}$, что способствует увеличению различности тем модели \cite{ARTM4}:
$$ R(\Phi) = -\dfrac{\tau}{2}\sum_{t \in T}\sum_{s \in T\setminus t} \text{cov}(\phi_t, \phi_s) \rightarrow 
\max\limits_{\Phi, \Theta}, \  \text{cov}(\phi_t, \phi_s) = \sum_{w \in W}\phi_{wt}\phi_{ws}.$$


\section{Иерархическая модель ARTM}

Тематическая иерархия ARTM представляет собой многоуровневый граф, где каждый уровень --- это плоская тематическая модель.
Для ее построения необходимо не только строить тематические модели уровней, но и устанавливать связи родитель-ребенок между темами соседних уровней. Для этого вводятся специальные межуровневые регуляризаторы. 

Пусть построено $l \geq 1$ уровней тематической иерархии, параметры $l$-того уровня --- матрицы $\Phi^l, \ \Theta^l$, $A$ --- множество тем $l$-го уровня. Построим $(l+1)$-ый уровень с параметрами $\Phi$, $\Theta$ и множеством тем $T$.

Будем моделировать распределение токенов по темам $l$-того уровня как смесь распределений по темам $(l+1)$-го уровня \cite{hARTM}:
$$p(w|a) \approx \sum\limits_{t \in T}  p(w|t)p(t|a) \ \ a\in A, w \in W.$$
Это приводит к задаче факторизации
\begin{align}
\Phi^l \approx \Phi \Psi, \tag{**}\label{hf}
\end{align}
где $\Phi = [p(w|t)]_{W \times T}, \Psi=[p(t|a)]_{T \times A}$. Полученная матрица $\Psi$ содержит распределения тем $(l+1)$-го уровня в темах $l$-го уровня. 
Таким образом, межуровневый регуляризатор --- это логарифм правдоподобия для задачи (\ref{hf}): $$R(\Phi, \Psi) = \sum\limits_{a \in A} \sum\limits_{w \in W} n_{wa} \ln{\sum\limits_{t \in T} \phi_{wt}\psi_{ta}}\rightarrow 
\max\limits_{\Phi, \Psi},$$ где $n_{a} = \sum\limits_{w \in W} n_{wa}$ --- веса тем родительского уровня, пропорциональные их размеру.


Этот регуляризатор эквивалентен добавлению в коллекцию $|A|$ псевдодокументов, представленных матрицей $[n_{wa}]_{W\times A}$. Тогда $\Psi$ образует $|A|$ дополнительных столбцов матрицы $\Theta$, соответсвующих этим псевдодокументам.

\section{Метрики качества тематических моделей}
