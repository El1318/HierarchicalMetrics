\chapter{Обзор литературы}

\section{Постановка задачи тематического моделирования}
В вероятностном тематическом моделировании коллекция документов рассматривается как множество троек $(d, w, t)$, выбранных случайно и независимо из дискретного распределения $p(d, w, t)$, заданного на конечном множестве $D \times W \times T$. Здесь $D$ -- множество документов коллекции, $W$ -- словарь, $T$ -- множество тем. Документы $d \in  D$ и токены $w \in W$ являются наблюдаемыми переменными, а тема $t \in T$ является латентной (скрытой) переменной.

Построить тематическую модель коллекции документов $D$ — значит найти
распределения $p(w |t)$ для всех тем $t \in T$ и распределения $p(t| d)$  для всех документов $d \in  D$. Известными при это являются распределения $p(w|d)$ терминов 
(токенов) в документах коллекции.

Предполагается, что порядок токенов $w \in W$ в документе $d \in D$ не важен (гипотеза \lq\lq{мешка слов}\rq\rq{}), что позволяет представить коллекцию в виде матрицы $[n_{dw} ]_{D \times W}$, где $n_{dw}$ --- число вхождений $w$ в $d$. Также коллекцию можно представить в виде матрицы эмпирических оценок вероятности встретить токен в документе $[p_{wd}]_{W\times D}$:
	$$p_{wd}=\hat p(w|d) = \dfrac {n_{dw}}{n_d},$$
 	где $n_d = \sum\limits_{w \in W} n_{dw}$ --- число слов в документе $d$. 

Кроме того, принимается гипотеза условной независимости: $$p(w|t,d)=p(w|t).$$ То есть вероятность появления токена в некоторой теме не зависит от того, в каком документе встретился этот токен.

\section{Плоские тематические модели}

\subsection{Вероятностный латентный семантический анализ}

При сделанных предположениях плоская (одноуровневая) тематическая модель описывается формулой 

$$p(w|d) \approx \sum\limits_{t \in T}  p(w|t)p(t|d) \ \ d\in D, w \in W,
$$
которая следует из определения условной вероятности и формулы полной вероятности.
Пусть число тем $|T|$ много меньше числа документов $|D|$ и числа терминов $|W|$. Тогда представим задачу в виде факторизации матрицы $F =[p_{wd}]_{W \times D} $: $$F \approx \Phi \Theta.$$
Параметры модели --- матрицы $\Phi = [\phi_{wt}]_{W \times T}, \ \phi_{wt}=p(w|t)$ (вероятности токенов в темах) и $\Theta = [\theta_{td}]_{T \times D}, \ \theta_{td}=p(t|d)$ (вероятности тем в документах). 

Первая модель, использующая описанный подход, называется вероятностный латентный семантический анализ (PLSA) \cite{PLSA}. В ней матрицы $\Phi$ и $\Theta$ находятся с помощью максимизации логарифма правдоподобия: 
	
	$$\sum\limits_{d \in D} \sum\limits_{w \in W} n_{dw} \ln{\sum\limits_{t \in T} \phi_{wt}\theta_{td}} \rightarrow 
	\max\limits_{\Phi, \Theta} $$
	
	$$ \text{при условиях}  \ \phi_{wt} \geq 0, \ \theta_{td} \geq 0,\ \sum\limits_{w \in W} \phi_{wt} = 1, \ \sum\limits_{t \in T} \theta_{td} = 1.$$
		
Эта оптимизационная задача решается EM-алгоритмом:

E-шаг: $$p(t|d, w) = \dfrac{p(w, t| d)}{p(w|d)} = \dfrac{p(w|t)p(t|d)}{p(w|d)} =\dfrac{\phi_{wt}\theta_{td}}{\sum\limits_{s\in T}\phi_{ws}\theta_{sd}}= p_{tdw}.$$

M-шаг: $$\phi_{wt} = \dfrac{n_{wt}}{\sum\limits_{v \in W}n_{vt}}; \ \  \ \ n_{wt} = \sum\limits_{d \in D} n_{dw} p_{tdw};$$
	$$ \theta_{td} = \dfrac{n_{td}}{\sum\limits_{s \in T}n_{sd}}; \ \  \ \ n_{td} = \sum\limits_{w \in d} n_{dw} p_{tdw}.$$

Задача является некорректно поставленной, так как она допускает бесконечное множество решений. Действительно, для любой матрицы $S$ ранга $|T|$ имеем $$\Phi \Theta = (\Phi S)(S^{-1}\Theta).$$

\subsection{Байесовский подход}
Байесовский подход к задаче тематического моделирования рассматривает оптимизационную задачу как максимизацию апостериорной вероятности. Тогда параметры модели генерируются из некоторых априорных распределений.

 Наиболее применимой моделью является латентное размещение Дирихле (LDA) \cite{LDA}, в которой вектора $\phi_t$ и $\theta_d$ генерируются из распределений Дирихле. Кроме того, было предложено множество обобщений LDA, учитывающих дополнительные требования к модели \cite{Chemudugunta2006, Rosen-Zvi2004, Than2012}.
 
\subsection{Аддитивная регуляризация тематических моделей}

Априорным распределениям параметров тематической модели соответствуют регуляризаторы в задаче оптимизации. Аддитивная регуляризация тематических моделей (модель ARTM) \cite{ARTM4} позволяет одновременно учитывать множество дополнительных требований к модели. 

В ARTM матрицы $\Phi$ и $\Theta$ находятся с помощью максимизации  суммы логарифма правдоподобия и регуляризаторов $R_i$ с неотрицательными коэффициентами регуляризации
$\tau_i$: 
$$ \sum\limits_{d \in D} \sum\limits_{w \in W} n_{dw} \ln{\sum\limits_{t \in T} \phi_{wt}\theta_{td}} + \sum_{i} \tau_i R_i(\Phi, \Theta) \rightarrow 
\max\limits_{\Phi, \Theta}$$ 
$$ \text{при условиях}  \ \phi_{wt} \geq 0, \ \theta_{td} \geq 0,\ \sum\limits_{w \in W} \phi_{wt} = 1, \ \sum\limits_{t \in T} \theta_{td} = 1.$$

Как показано в \cite{ARTM4}, ЕМ-алгоритм для этой задачи имеет следующий вид:

E-шаг: $$p(t|d, w) = p_{tdw} = \norm\limits_{t\in T}(\phi_{wt}\theta_{td}).$$

M-шаг: $$\phi_{wt} = \norm\limits_{w \in W}\Bigl(n_{wt}+\phi_{wt}\sum\limits_{i}\tau_i\dfrac{\partial  R_i}{\partial \phi_{wt}}\Bigr); \ \  \ \ n_{wt} = \sum\limits_{d \in D} n_{dw} p_{tdw};$$
	$$ \theta_{td} = \norm\limits_{t \in T}\Bigl(n_{td}+\theta_{td}\sum\limits_{i}\tau_i\dfrac{\partial R_i}{\partial \theta_{td}}\Bigr); \ \  \ \ n_{td} = \sum\limits_{w \in d} n_{dw} p_{tdw},$$

где введен оператор $ \norm\limits_{t\in T} (x_t) = \dfrac{\max\{x_t, 0\}}{\sum\limits_{s\in T} \max\{x_s, 0\}}$.

Тогда модель PLSA эквивалентна модели ARTM в случае $R(\Phi, \Theta) = \sum_{i} \tau_i R_i(\Phi, \Theta) = 0$.

Регуляризаторы ARTM не обязаны иметь вероятностные интерпретации и могут учитывать любые особенности модели. Приведем описание регуляризаторов, используемых в данной работе.

\textbf{Сглаживание и разреживание}:

Регуляризатор сглаживания вводит в модель требование, чтобы столбцы $\phi_{t}$ и $\theta_{d}$ были близки к заданным распределениям $\beta_t = [\beta_{wt}]_{w \in W}$ и $\alpha_d = [\alpha_{td}]_{t \in T}$ в смысле дивиргенции Кульбака-Лейблера \cite{ARTM4}:
$$R(\Phi, \Theta) = \beta_0\sum_{m\in M} \sum_{t\in T}\sum_{w \in W^m} \beta_{wt}\ln \phi_{wt} + \alpha_0 \sum_{d\in D} \sum_{t\in T}\alpha_{td} \ln \theta_{td} \rightarrow 
\max\limits_{\Phi, \Theta},$$
где $\beta_0$ и $\alpha_0$ --- заданные положительные коэффициенты. Введение этого регуляризатора, как показано в \cite{ARTM4}, эквивалентно модели LDA.

Регуляризатор разреживания имеет такой же вид, но коэффициенты $\beta_0$ и $\alpha_0$ отрицательны. Он способствует обращению значительной части вероятностей $\phi_{wt}$ и $\theta_{td}$ в ноль, что соответствует ествественному предположению о том, что каждый документ $d$ и каждый токен $w$ связаны лишь с небольшим числом тем $t$.


\textbf{Декоррелирование:}

Регуляризатор декоррелирования минимизирует ковариации между столбцами $\phi_{t}$, что способствует увеличению различности тем модели \cite{ARTM4}:
$$ R(\Phi) = -\dfrac{\tau}{2}\sum_{t \in T}\sum_{s \in T\setminus t} \text{cov}(\phi_t, \phi_s) \rightarrow 
\max\limits_{\Phi, \Theta}, \  \text{cov}(\phi_t, \phi_s) = \sum_{w \in W}\phi_{wt}\phi_{ws}.$$

\subsection{Мультимодальные тематические модели}
Документы $d \in D$ могут содержать не только текст, но и другие элементы, такие как тэги, ссылки, имена авторов и т.д. Такие типы элементов называются модальностями. Пусть $M$ --- множество модальностей. Каждой $m \in M$ соответсвует отдельный словарь (множество токенов)  $W^m$, причем $W = \bigsqcup\limits_{m \in M} W^m $.

Пусть $F^m=[p_{wd}]_{W^m \times D}$, $m \in M$ --- матрицы наблюдаемых вероятностей для каждой модальности, а $\Phi^m$ --- соответсвующие матрицы скрытых вероятностей $p(w|t)$. Определим $F$ и $\Phi$ как объединения строк $F^m$ и $\Phi^m, \ m \in M$ соответственно. Тогда получим задачу факторизации $F \approx \Phi \Theta$ для мультимодальной тематической модели.

При построении мультимодальной тематической модели максимизируется взвешенная сумма логарифмов правдоподобия для всех модальностей $m \in M$ и регуляризаторов $R_i$ \cite{ARTM4}: 
$$ \sum\limits_{m \in M} \kappa_m \sum\limits_{d \in D} \sum\limits_{w \in W^m} n_{dw} \ln{\sum\limits_{t \in T} \phi_{wt}\theta_{td}} + \sum_{i} \tau_i R_i(\Phi, \Theta) \rightarrow 
\max\limits_{\Phi, \Theta}$$
$$ \text{при условиях}  \ \phi_{wt} \geq 0, \ \theta_{td} \geq 0,\ \sum\limits_{w \in W} \phi_{wt} = 1, \ \sum\limits_{t \in T} \theta_{td} = 1.$$




\section{Иерархические тематические модели}
Тематическая иерархия представляет собой многоуровневый граф, где каждый уровень --- это плоская тематическая модель. Для ее построения необходимо не только строить тематические модели уровней, но и устанавливать связи родитель-ребенок между темами соседних уровней. При этом общепринятого определения и подхода к построению иерархических тематических моделей не существует. 

\subsection{Обобщения LDA для тематических иерархий}
 Многие иерархические тематические модели были разработаны как обобщения модели LDA. Первой такой моделью являлялось иерархическое LDA (hLDA)  \cite{hLDA}. В нем темы образуют дерево, то есть каждая подтема (тема-ребенок) имеет только одну тему-родителя. 
 
 С другой стороны, модель иерархического распределения патинко (hPAM) \cite{hPAM} представляет собой направленный ациклический многодольный граф. Модель иерархического процесса Дирихле (hHDP) \cite{hHDP} также является многодольным графом и дополнительно
	 обеспечивает возможность оценивать количество уровней и количество тем на каждом уровне иерархии.

Существуют масштабируемые модели тематических иерархий, которые подходят для больших наборов данных, например \cite{Pujara2012, Zhai2012}.
\subsection{Иерархическая модель ARTM}
 В иерархической модели ARTM (hARTM) \cite{hARTM} для связи уровней иерархии вводятся специальные межуровневые регуляризаторы. При этом иерархия является многодольным ациклическим графом.

Пусть построено $l \geq 1$ уровней тематической иерархии, параметры $l$-того уровня --- матрицы $\Phi^l, \ \Theta^l$, $A$ --- множество тем $l$-го уровня. Построим $(l+1)$-ый уровень с параметрами $\Phi$, $\Theta$ и множеством тем $T$.

Будем моделировать распределение токенов по темам $l$-того уровня как смесь распределений по темам $(l+1)$-го уровня \cite{hARTM}:
$$p(w|a) \approx \sum\limits_{t \in T}  p(w|t)p(t|a) \ \ a\in A, w \in W.$$
Это приводит к задаче факторизации
\begin{align}
\Phi^l \approx \Phi^{l+1} \Psi^{l}, \notag
\end{align}
где $\Phi^{l+1} = [p(w|t)]_{W \times T}, \Psi^{l}=[p(t|a)]_{T \times A}$. Полученная матрица $\Psi$ содержит распределения тем $(l+1)$-го уровня в темах $l$-го уровня. 
Таким образом, межуровневый регуляризатор --- это логарифм правдоподобия для этой задачи факторизации: $$R(\Phi, \Psi) = \sum\limits_{a \in A} \sum\limits_{w \in W} n_{wa} \ln{\sum\limits_{t \in T} \phi_{wt}\psi_{ta}}\rightarrow 
\max\limits_{\Phi, \Psi},$$ где $n_{a} = \sum\limits_{w \in W} n_{wa}$ --- веса тем родительского уровня, пропорциональные их размеру.


Этот регуляризатор эквивалентен добавлению в коллекцию $|A|$ псевдодокументов, представленных матрицей $[n_{wa}]_{W\times A}$. Тогда $\Psi$ образует $|A|$ дополнительных столбцов матрицы $\Theta$, соответсвующих этим псевдодокументам.

\section{Метрики качества тематических моделей}
Существуют общепринятые в тематическом моделировании методы оценки качества тем. Большинство предложенных методов используют некоторое фиксированное количество $n$ наиболее вероятных токенов темы (топ-токенов) $w_i^{(t)}$, где $i \in \{1,...,n\}, \ t \in T$ и некоторую функцию близости $f(\cdot, \cdot)$ этих токенов: 

$$\mathrm{Q}(t) = \dfrac{1}{n^2}\sum\limits_{i=1}^n\sum\limits_{j=1}^n f(w_i^{(t)}, w_j^{(t)}),$$

В \cite{Mimno2011} предложена мера когерентности темы, основанная на совстречаемости топ-токенов в некоторой коллекции:
$$C(t) = \dfrac{1}{n^2}\sum\limits_{i=1}^n \sum\limits_{j=1}^n \ln \dfrac{D(w^{(t)}_i, w^{(t)}_j) + \varepsilon}{D(w^{(t)}_j)}[w_i \neq w_j].$$ 

Здесь $D(w_1, w_2)$ -- количество документов в некотором корпусе, где слова $w_1$ и $w_2$ встретились вместе, а $D(w)$ -- количество документов, в которых встречается токен $w$. Для подсчета совстречаемостей предпочтительно использовать большие внешние текстовые коллекции. Когерентность измеряет синтагматическую родственность токенов темы \cite{Schutze1993}.

В работе \cite{Nikolenko2017} предложена модификация когерентности, называемая tf-idf когерентностью:

 $$C_{tfidf}(t) = \dfrac{1}{n^2}\sum\limits_{i=1}^n \sum\limits_{j=1}^n \ln{\frac{\sum\limits_{d: w_i \in d, w_j \in d} \mathrm{tfidf}(w_i, d)\mathrm{tfidf}(w_j, d) + \epsilon}{\sum\limits_{d: w_i \in d}\mathrm{tfidf}(w_i, d)}}[w_i \neq w_j].$$


Эта метрика помогает решить проблему того, что классическая когерентность
слишком сильно полагаться на общие слова, которые часто встречаются в коллекции,
но не определяют интерпретируемые темы.

Еще одна метрика такого же типа предложена в \cite{Nikolenko2016}. В ней мерой близости токенов является расстояние между их векторными представлениями, то есть векторами модели word embedding:

$$C_{emb}(t) = -\dfrac{1}{n^2}\sum\limits_{i=1}^n \sum\limits_{j=1}^n d( v_{w_i}, v_{w_j})[w_i \neq w_j].$$
Здесь $v_{w}$ -- вектор, соответствующий токену $w$ в пространстве word embedding, $d(\cdot, \cdot)$ -- некоторая функция расстояния между векторами. Метрика, основанная на векторном представлении слов, оценивает парадигматическую родственность токенов темы \cite{Schutze1993}.

Другой тип метрик качества основан на поточечной взаимной информации (PMI). Следуя \cite{Lau2014}, рассмотрим три метрики этого типа.

В \cite{Newman2010} используется расстояние попарного PMI:

$$PMI(t) =  \dfrac{1}{n^2}\sum\limits_{i=1}^n \sum\limits_{j=1}^n  \ln{\frac{p(w_i, w_j)}{p(w_i) p(w_j)}}[w_i \neq w_j]$$

В \cite{Bouma2009} используется расстояние нормализованного PMI:

$$NPMI(t) =\dfrac{1}{n^2}\sum\limits_{i=1}^n \sum\limits_{j=1}^n   \dfrac{\ln{\frac{p(w_i, w_j)}{p(w_i) p(w_j)}}}{-\ln p(w_i, w_j)}[w_i \neq w_j].$$

В \cite{Mimno2011} используется попарная условная вероятность:

$$LCP(t) = \dfrac{1}{n^2}\sum\limits_{i=1}^n \sum\limits_{j=1}^n \ln\frac{p(w_i, w_j)}{p(w_i)} [w_i \neq w_j].$$

Главное требование, предъявляемое к метрике качества темы, -- согласованность с человеческими оценками. Хорошим качеством согласно метрике должны обладать темы, хорошо интерпретируемые с точки зрения человека, и наоборот. Для проверки того, действительно ли некоторая метрика оценивает интерпретируемость темы, проводят асессорские эксперименты, в которых собирают мнения людей о качестве некоторого набора тем. Далее проводится сравнение собранных оценок со значениями метрики на темах из эксперимента. Таким образом построены эксперименты, например, в \cite{Mimno2011} и \cite{Nikolenko2016}.

В \cite{Nikolenko2016} проведено сравнение согласованности с асессорами всех приведенных метрик качества тем, включая вариации метрики $C_{emb}(t)$ с использованием различных функций расстояния. Эксперимент проводился на русскоязычных коллекциях, для векторного представления слов использовалась модель word2vec, обученная на большом русскоязычном корпусе из \cite{Panchenko2015}. Самые высокие значения согласованности с асессорами показала метрика $C_{emb}(t)$.


 

 







