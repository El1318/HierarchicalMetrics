\chapter{Агрегирование гетерогенных текстовых коллекций в тематической иерархии}
\section{Постановка задачи}
\section{Описание данных}
Вычислительный эксперимент в данной работе должен проводиться на текстовых коллекциях из нескольких источников, которые существенно отличаются друг от друга по размеру и набору тем. Такие коллекции позволяют исследовать работу предлагаемого метода на гетерогенных данных.

 Использовались коллекции статей Хабрахабра и научно-популярного интернет-журнала ПостНаука.
Коллекция ПостНауки состоит из 2976 документов и содержит модальности слов (43196 токенов) и тэгов (1799 токенов). Коллекция Хабрахабра состоит из 81076 документов
и содержит модальность слов, содержащихся в коллекции ПостНауки (35640 токенов), модальность слов, уникальных для Хабрахабра (545200 токенов),
модальность тэгов, содержащихся в коллекции ПостНауки (673 токена), модальность тэгов, уникальных для Хабрахабра (76429 токенов).

Коллекция ПостНауки содержит разнородные темы (технические, естественнонаучные и гуманитарные), поэтому для большинства научно-популярных статей в ней найдутся тематически близкие документы. При этом она сравнительно мала по объему.
Коллекция Хабрахабра напротив содержит в основном технические статьи, меньше естественнонаучных и почти не содержит гуманитарных. При этом объем коллекции большой.

Предобработка данных включает в себя нормализацию текста: перевод в нижний регистр, токенизацию и лемматизацию, удаление редко (менее двух раз) встречающихся слов, удаление наиболее часто встречающихся слов, удаление знаков пунктуации. Кроме того, из коллекций были удалены все статьи, содержащие менее 100 предобработанных слов.
\section{Базовый алгоритм}
Стандартный алгоритм построения тематической модели гетерогенных текстовых коллекций предполагает объединение коллекций из разных источников и построение общей модели. В этом эксперименте объединяются коллекции ПостНауки и Хабрахабра.

В силу того, что размер коллекции Хабрахабра существенно превосходит размер коллекции ПостНауки, в большинстве тем полученной модели более $90\%$ статей относятся к Хабрахабру.

\begin{table}[h]
\centering
\caption{Доля статей Хабрахабра в темах модели объединенной коллекции}
\vspace{1ex}
\begin{tabular}{|l|*{10}{c|}r|}
\hline
Номер темы & 0 & 1 & 2 & 3 & 4   \\
\hline
Доля статей Хабрахабра & 0.941 &  0.790 & 0.967 & 0.983 & 0.912  \\
\hline
\hline
Номер темы	&5 & 6 & 7 & 8 & 9 \\
\hline
Доля статей Хабрахабра & 0.950 & 0.980& 0.994 & 0.922 &  0.953  \\
\hline
\hline
Номер темы & 10 & 11 & 12  &13 & 14\\
\hline
Доля статей Хабрахабра & 0.976 &  0.905 & 0.987 &  0.926 &  0.977  \\
\hline
\hline
Номер темы & 15 & 16 & 17  &18 & 19   \\
\hline
Доля статей Хабрахабра & 0.996 & 0.969 &  0.969  &0.991 & 0.965 \\
\hline
\end{tabular}
\end{table}

Поэтому модель объединенной коллекции отражает в основном тематическую структуру Хабрахабра. Все темы имеют технический характер, характерный для него. При этом потеряны гуманитарные темы, представленные в ПостНауке.
Кроме того, объединенная коллекция имеет большой размер, поэтому для построения ее модели требуется намного больше времени, чем для построения модели ПостНауки.
Таким образом, базовый алгоритм не решает поставленную задачу и требует модификаций.
\section{Предлагаемый алгоритм}

В данной статье предлагается проводить достроение модели в два этапа.
На первом этапе проводится фильтрация коллекции, которую необходимо добавить в существующую модель. На втором этапе прошедшие фильтрацию документы добавляются в существующую модель.

\subsection{Фильтрация новой коллекции}
Целью этого этапа является сокращение объема добавляемой коллекции (отфильтрованная коллекция должна содержать меньше документов, чем уже содержится в построенной модели) и удаление статей, которые далеки по содержанию от уже присутсвующих в модели. Это позволяет отобрать статьи, которые необходимо агрегировать и отсеять те, которые являются нерелевантными. При этом в силу меньшего объема отфильтрованной коллекции
возможно сохранить все существующие темы и дополнить те из них, которые наиболее характерны для добавляемой коллекции.

Фильтрацию предлагается проводить по доле слов в статье, присутствующих в словаре существующей модели, и расстоянию до ближайших статей из старой коллекции. Расстояние между документами будем понимать как косинусное расстояние между их tf-idf-представлениями, где idf берется по старой коллекции.

Пусть $\boldsymbol{d_n }= [ \text{tf(term}_i, d_n) \cdot \text{idf(term}_i, D) ]_{i=1}^{|W|}$, где $D$ --- старая коллекция, $D_{\text{new}}$ --- добавляемая коллекция, $\text{term}_i \in W$ --- токены из старого словаря $W$, $d_n \in D_{\text{new}} \cup D$ --- документ.

Расстояние между документами $d_n$ и $d_m$ рассчитывается как их косинусное сходство:
$$\text{dist}(d_n, d_m) = \dfrac{\boldsymbol{d_n }^T \boldsymbol{d_m }}{||\boldsymbol{d_n }|| \cdot ||\boldsymbol{d_m }||}.$$

Доля неуникальных для нового источника слов в документе $d_n \in D_{\text{new}}$ рассчитывается по формуле
$$P_{\text{common}}(d_n) = \dfrac{n_{\text{common}}(d_n)}{n_{\text{common}}(d_n)+n_{\text{unique}}(d_n)},$$ где $n_{\text{common}}$ --- количество слов из словаря существующей модели в статье, $n_{\text{unique}}$ --- количество уникальных для источника слов в статье.

Пусть $\text{Dists} = [ \text{dist}(d_n, d_m)]_{d_n \in D_{\text{new}}, d_m \in D }$ --- матрица расстояний между документами из новой и старой коллекций. Тогда искомая отфильтрованная выборка состоит из тех документов $d_n$, для которых выполняется условие

\begin{equation}
 \begin{cases}
   P_{\text{common}}(d_n) > \text{threshold}_1 \\
   \\
   \text{mean}_{m \in [m_1, ..., m_{10}]}(\text{Dists}[n,m])< \text{threshold}_2,
   \end{cases} \notag
\end{equation}
где $[m_1, ..., m_{10}]$ --- индексы, соответствующие 10 наименьшим значениям $\text{Dists}[n,:]$, $\text{mean}(\cdot)$ --- среднее арифметическое,   $\text{threshold}_i, i \in [1,2]$ --- заданные пороги.

\subsection{Дополнение модели отфильтрованной коллекцией}
Полученная выборка документов добавляется в существующую модель.
Для этого объединим старую коллекцию с этой выборкой.

\textbf{Первый уровень иерархии:}
Предполагается, что первый уровень иерархической тематической модели содержит все темы, характерные для агрегируемого контента. Тогда после добавления новых документов строки новой матрицы $\Phi^{1}_{\text{new}}$ первого уровня иерархии, соответсвующие токенам старого словаря, не должны значительно отличаться от соответствующих строк старой матрицы $\Phi^{1}$. Поэтому инициализируем эту подматрицу $\Phi^{1}_{\text{new}}$ матрицей $\Phi^{1}$, а остальные строки инциализируем случайно. Для того чтобы темы не изменились значительно, применим регуляризатор сглаживания $\Phi^{1}$ по всем инициализированным темам.

\textbf{Второй уровень иерархии:}
Предполагается, что второй уровень иерархии содержит более специфические темы, чем первый уровень. Тогда добавление новых документов в коллекцию может привести к появлению подтем, характерных для нового источника, на втором уровне. Поэтому добавим в модель некоторое фиксированное количество новых тем. Инициализируем подматрицу $\Phi^{2}_{\text{new}}$, соответствующую старым токенам и темам, старой матрицей $\Phi^{2}$, как и на первом уровне. Новые токены и темы инициализируем случайно. Для сохранения инициализированных тем применяем регуляризатор сглаживания $\Phi$ по ним.

Новые темы должны быть специфичными для нового источника, то есть большинство отнесенных к ним документов должны принадлежать новой коллекции. Поэтому применим регуляризатор разреживания $\Theta$ по новым темам для документов старой коллекции.

\section{Сравнение алгоритмов}