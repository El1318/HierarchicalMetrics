\chapter{Агрегирование гетерогенных текстовых коллекций в тематической иерархии}
\section{Постановка задачи}

В данной работе решается задача агрегирования больших гетерогенных текстовых коллекций в их общей иерархической тематической модели.
Предполагается, что есть начальное приближение тематической иерархии, в котором уже присутствует большинство агрегируемых тем. Назовем коллекцию, модель которой берется за начальное приближение, базовой. Такая модель должна быть хорошо интерпретируемой.

Алгоритм, решающий поставленную задачу, должен позволить увеличивать объем и разнообразие агрегируемого контента, добавляя в модель базовой коллекции документы из других источников. При этом конечный размер агрегируемой коллекции может во много раз превышать размер базовой коллекции. Кроме того, модель должна оставаться интерпретируемой и как можно меньше терять в качестве по сравнению с моделью базовой коллекции. 

Вычислительный эксперимент в данной работе должен проводиться на текстовых коллекциях из нескольких источников, которые существенно отличаются друг от друга по размеру и набору тем. Такие коллекции позволяют исследовать работу предлагаемого метода на гетерогенных данных.

В качестве примера в этой работе рассматривается задача агрегирования русскоязычного научно-популярного контента из коллекций, описанных в секции 3.2.1.
 Будем использовать коллекцию ПостНауки в качестве базовой, так как она содержит разнородные темы (технические, естественнонаучные и гуманитарные), поэтому для большинства научно-популярных статей в ней найдутся тематически близкие документы. При этом она сравнительно мала по объему.
Коллекция Хабрахабра напротив содержит в основном технические статьи, меньше естественнонаучных и почти не содержит гуманитарных. При этом ее объем во много раз превышает объем коллекции ПостНауки, взятой за базовую. Таким образом, на примере этих коллекций можно оценить эффективность алгоритмов для задачи агрегации.

\section{Базовый алгоритм}

Стандартный алгоритм построения тематической модели гетерогенных текстовых коллекций предполагает объединение коллекций из разных источников и построение общей модели. В этом эксперименте объединяются коллекции ПостНауки и Хабрахабра.

В силу того, что размер коллекции Хабрахабра существенно превосходит размер коллекции ПостНауки, в большинстве тем полученной модели более 90\% статей относятся к Хабрахабру.

\begin{table}[h!]
\centering
\vspace{1ex}
\begin{tabular}{l|r|r|r|r|r}
\hline
Номер темы & 0 & 1 & 2 & 3 & 4   \\
\hline
Доля статей Хабрахабра & 0.941 &  0.790 & 0.967 & 0.983 & 0.912  \\
\hline
\hline
Номер темы	&5 & 6 & 7 & 8 & 9 \\
\hline
Доля статей Хабрахабра & 0.950 & 0.980& 0.994 & 0.922 &  0.953  \\
\hline
\hline
Номер темы & 10 & 11 & 12  &13 & 14\\
\hline
Доля статей Хабрахабра & 0.976 &  0.905 & 0.987 &  0.926 &  0.977  \\
\hline
\hline
Номер темы & 15 & 16 & 17  &18 & 19   \\
\hline
Доля статей Хабрахабра & 0.996 & 0.969 &  0.969  &0.991 & 0.965 \\
\hline
\end{tabular}
\caption{Доля статей Хабрахабра в темах модели объединенной коллекции}
\end{table}

Поэтому модель объединенной коллекции отражает в основном тематическую структуру Хабрахабра. Все темы имеют технический характер, характерный для него. При этом потеряны гуманитарные темы, представленные в ПостНауке.
Кроме того, объединенная коллекция имеет большой размер, поэтому для построения ее модели требуется намного больше времени, чем для построения модели ПостНауки.
Таким образом, базовый алгоритм не решает поставленную задачу и требует модификаций.

\section{Предлагаемый алгоритм}

В данной статье предлагается проводить достроение модели в два этапа.
На первом этапе проводится фильтрация коллекции, которую необходимо добавить в существующую модель. На втором этапе прошедшие фильтрацию документы добавляются в существующую модель.

\subsection{Фильтрация новой коллекции}
Целью этого этапа является сокращение объема добавляемой коллекции (отфильтрованная коллекция должна содержать меньше документов, чем уже содержится в построенной модели) и удаление статей, которые далеки по содержанию от уже присутсвующих в модели. Это позволяет отобрать статьи, которые необходимо агрегировать и отсеять те, которые являются нерелевантными. При этом в силу меньшего объема отфильтрованной коллекции
возможно сохранить все существующие темы и дополнить те из них, которые наиболее характерны для добавляемой коллекции.

Фильтрацию предлагается проводить по доле слов в статье, присутствующих в словаре существующей модели, и расстоянию до ближайших статей из старой коллекции. Расстояние между документами будем понимать как косинусное расстояние между их tf-idf-представлениями, где idf берется по старой коллекции.

Пусть $\boldsymbol{d_n }= [ \text{tf(term}_i, d_n) \cdot \text{idf(term}_i, D) ]_{i=1}^{|W|}$, где $D$ --- старая коллекция, $D_{\text{new}}$ --- добавляемая коллекция, $\text{term}_i \in W$ --- токены из старого словаря $W$, $d_n \in D_{\text{new}} \cup D$ --- документ.

Расстояние между документами $d_n$ и $d_m$ рассчитывается как их косинусное сходство:
$$\text{dist}(d_n, d_m) = \dfrac{\boldsymbol{d_n }^T \boldsymbol{d_m }}{||\boldsymbol{d_n }|| \cdot ||\boldsymbol{d_m }||}.$$

Доля неуникальных для нового источника слов в документе $d_n \in D_{\text{new}}$ рассчитывается по формуле
$$P_{\text{common}}(d_n) = \dfrac{n_{\text{common}}(d_n)}{n_{\text{common}}(d_n)+n_{\text{unique}}(d_n)},$$ где $n_{\text{common}}$ --- количество слов из словаря существующей модели в статье, $n_{\text{unique}}$ --- количество уникальных для источника слов в статье.

Пусть $\text{Dists} = [ \text{dist}(d_n, d_m)]_{d_n \in D_{\text{new}}, d_m \in D }$ --- матрица расстояний между документами из новой и старой коллекций. Тогда искомая отфильтрованная выборка состоит из тех документов $d_n$, для которых выполняется условие

\begin{equation}
 \begin{cases}
   P_{\text{common}}(d_n) > \text{threshold}_1 \\
   \\
   \text{mean}_{m \in [m_1, ..., m_{10}]}(\text{Dists}[n,m])< \text{threshold}_2,
   \end{cases} \notag
\end{equation}
где $[m_1, ..., m_{10}]$ --- индексы, соответствующие 10 наименьшим значениям $\text{Dists}[n,:]$, $\text{mean}(\cdot)$ --- среднее арифметическое,   $\text{threshold}_i, i \in [1,2]$ --- заданные пороги.

\subsection{Дополнение модели отфильтрованной коллекцией}
Полученная выборка документов добавляется в существующую модель.
Для этого объединим старую коллекцию с этой выборкой.

\textbf{Первый уровень иерархии:}
Предполагается, что первый уровень иерархической тематической модели содержит все темы, характерные для агрегируемого контента. Тогда после добавления новых документов строки новой матрицы $\Phi^{1}_{\text{new}}$ первого уровня иерархии, соответсвующие токенам старого словаря, не должны значительно отличаться от соответствующих строк старой матрицы $\Phi^{1}$. Поэтому инициализируем эту подматрицу $\Phi^{1}_{\text{new}}$ матрицей $\Phi^{1}$, а остальные строки инциализируем случайно. Для того чтобы темы не изменились значительно, применим регуляризатор сглаживания $\Phi^{1}$ по всем инициализированным темам.

\textbf{Второй уровень иерархии:}
Предполагается, что второй уровень иерархии содержит более специфические темы, чем первый уровень. Тогда добавление новых документов в коллекцию может привести к появлению подтем, характерных для нового источника, на втором уровне. Поэтому добавим в модель некоторое фиксированное количество новых тем. Инициализируем подматрицу $\Phi^{2}_{\text{new}}$, соответствующую старым токенам и темам, старой матрицей $\Phi^{2}$, как и на первом уровне. Новые токены и темы инициализируем случайно. Для сохранения инициализированных тем применяем регуляризатор сглаживания $\Phi$ по ним.

Новые темы должны быть специфичными для нового источника, то есть большинство отнесенных к ним документов должны принадлежать новой коллекции. Поэтому применим регуляризатор разреживания $\Theta$ по новым темам для документов старой коллекции.

\section{Сравнение алгоритмов}

