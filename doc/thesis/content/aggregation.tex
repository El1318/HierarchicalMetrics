\chapter{Агрегирование гетерогенных текстовых коллекций в тематической иерархии}
\section{Постановка задачи}

В данной работе решается задача агрегирования больших гетерогенных текстовых коллекций в их общей иерархической тематической модели.
Предполагается, что есть начальное приближение тематической иерархии, в котором уже присутствует большинство агрегируемых тем. 

Назовем коллекцию, модель которой берется за начальное приближение, базовой. Такая модель должна быть хорошо интерпретируемой.
Алгоритм, решающий поставленную задачу, должен позволить увеличивать объем и разнообразие агрегируемого контента, добавляя в модель базовой коллекции документы из других источников. При этом конечный размер агрегируемой коллекции может во много раз превышать размер базовой коллекции. Кроме того, модель должна оставаться интерпретируемой и как можно меньше терять в качестве по сравнению с моделью базовой коллекции. 

Вычислительный эксперимент в данной работе должен проводиться на текстовых коллекциях из нескольких источников, которые существенно отличаются друг от друга по размеру и набору тем. Такие коллекции позволяют исследовать работу предлагаемого метода на гетерогенных данных.

В качестве примера в этой работе рассматривается задача агрегирования русскоязычного научно-популярного контента из коллекций, описанных в секции 3.2.1.
 Будем использовать коллекцию ПостНауки в качестве базовой, так как она содержит разнородные темы (технические, естественнонаучные и гуманитарные), поэтому для большинства научно-популярных статей в ней найдутся тематически близкие документы. При этом она сравнительно мала по объему.
Коллекция Хабрахабра напротив содержит в основном технические статьи, меньше естественнонаучных и почти не содержит гуманитарных. При этом ее объем во много раз превышает объем коллекции ПостНауки, взятой за базовую. Таким образом, на примере этих коллекций можно оценить эффективность алгоритмов для задачи агрегации.

\section{Базовый алгоритм}

Стандартный алгоритм построения тематической модели гетерогенных текстовых коллекций предполагает объединение коллекций из разных источников и построение общей модели. В этом эксперименте объединяются коллекции ПостНауки и Хабрахабра.

В силу того, что размер коллекции Хабрахабра существенно превосходит размер коллекции ПостНауки, в большинстве тем полученной модели более 90\% статей относятся к Хабрахабру.

\begin{table}[h!]
\centering
\vspace{1ex}
\begin{tabular}{l|r|r|r|r|r|r|r|r|r|r|r}

$t$ & 0 & 1 & 2 & 3 & 4 &5 & 6 & 7 & 8 & 9 & 10\\
\hline
$P_t$ & 0.94 &  0.79 & 0.97 & 0.98 & 0.91 & 0.95 & 0.98& 0.99 & 0.92 &  0.95 &  0.98 \\
\hline
\hline
$t$ & 10 & 11 & 12  &13 & 14 & 15 & 16 & 17  &18 & 19 & 20\\
\hline
$P_t$ & 0.98 &  0.91 & 0.99 &  0.93 &  0.98  & 1.0 & 0.97 &  0.97  &0.99 & 0.97 & 0.95\\
\end{tabular}
\caption{\label{table:baseline}Доля статей Хабрахабра в темах модели объединенной коллекции. $t$ -- номер темы, $P_t$ -- доля статей Хабрахабра в этой теме, если к теме относятся документы с $p(t|d) > 0.1$.}
\end{table}

Поэтому модель объединенной коллекции отражает в основном тематическую структуру Хабрахабра. Все темы имеют технический характер, характерный для него. При этом потеряны гуманитарные темы, представленные в ПостНауке.
Кроме того, объединенная коллекция имеет большой размер, поэтому для построения ее модели требуется намного больше времени, чем для построения модели ПостНауки.
Таким образом, базовый алгоритм не решает поставленную задачу и требует модификаций.

\section{Предлагаемый алгоритм}

В данной работе для решения поставленной задачи предлагается проводить построение модели в несколько этапов. Обозначим базовую коллекцию как $D_0$, а коллекцию, состоящую из документов, которые необходимо добавить к агрегируемому контенту, как $D_1$. Тогда предлагаемый метод можно описать следующим образом:

\begin{enumerate}
	\item \textbf{Начальное приближение}: построить модель базовой коллекции $D_0$. Параметры модели начального приближения обозначим как $\Phi_0^l$, $\Theta_0^l$, где $l$ -- номер уровня. 
	\item \textbf{Фильтрация}: отранжировать документы добавляемой коллекции $D_1$ в порядке уменьшения близости к базовой коллекции $D_0$ в смысле некоторой метрики.
	
	\item \textbf{Дополнение модели:} добавить выбранное количество документов $D_1$, отранжированных в начало списка на этапе фильтрации, в коллекцию. При этом получим новую коллекцию $D$. Инициализировать параметры $\Phi^l$ модели коллекции $D$ параметрами $\Phi_0^l$ начального приближения. Обучить иерархическую модель $D$.
	 
\end{enumerate}

Первый этап -- задача построения интерпретируемой тематической иерархии однородной коллекции. Эта задача рассмотрена в \cite{hARTM}, кроме того, в \cite{Ianina2017} рассмотрена задача подбора коэффициентов регуляризации в модели ARTM. 

Опишем подробнее два последних этапа.

\subsection{Фильтрация новой коллекции}
Целью этого этапа является сокращение объема добавляемой коллекции (отфильтрованная коллекция должна содержать меньше документов, чем уже содержится в построенной модели) и удаление статей, которые далеки по содержанию от уже присутсвующих в модели. Это позволяет отобрать статьи, которые необходимо агрегировать и отсеять те, которые являются нерелевантными.

Фильтрацию предлагается проводить по расстоянию новых документов до ближайших документов из старой коллекции. Это расстояние можно рассчитывать по разным метрикам близости документов. В этой работе будем использовать косинусное расстояние между их tf-idf-представлениями, где idf берется по старой коллекции.

Пусть $\boldsymbol{d_n }= [ \text{tf}(w_i, d_n) \cdot \text{idf}(w_i, D_0) ]_{i=1}^{|W|}$, где $D_0$ --- базовая коллекция, $D_1$ --- добавляемая коллекция, $w_i \in W$ --- токены из словаря базовой коллекции $W$, $d_n \in D_0 \cup D_1$ --- документ.

Расстояние между документами $d_n$ и $d_m$ рассчитывается как их косинусное сходство:
$$\rho(d_n, d_m) = \dfrac{\boldsymbol{d_n }^T \boldsymbol{d_m }}{||\boldsymbol{d_n }|| \cdot ||\boldsymbol{d_m }||}.$$

Для каждого документа $d_n \in D_1$ можно найти 10 ближайших документов из коллекции $D_0$. Пусть этим документам соответствуют индексы $[m_1, ..., m_{10}]$. Тогда искомая отфильтрованная выборка состоит из тех документов $d_n$, для которых выполняется условие

$$\tfrac{1}{10}\sum_{m \in [m_1, ..., m_{10}]}\rho(d_n, d_m) < t,$$
где $t$ -- некоторое заданное пороговое сходство между документами.

Обозначим отфильтрованную коллекцию, состоящую из документов, удовлетворяющих данному условию, как $D_f$.

\subsection{Дополнение модели отфильтрованной коллекцией}
На этом этапе полученная выборка документов добавляется в модель начального приближения иерархии. 
При этом предполагается, что начальное приближение уже содержит все темы, характерные для агрегируемого контента. Тогда процесс добавления новых документов можно описать следующим образом:

\begin{enumerate}
	\item \textbf{Слияние коллекций:} добавить отфильтрованную коллекцию к базовой и получить объединенную коллекцию $D = D_0 \cup D_f$.
	\item \textbf{Инициализация:} после добавления новых документов строки новых матриц $\Phi^l$ иерархии коллекции $D$, соответсвующие токенам старого словаря, не должны значительно отличаться от соответствующих строк старой матрицы $\Phi^l_0$. Будем строить модель, используя только словарь базовой коллекции. Поэтому инициализируем матрицы $\Phi^l$ соответствующими матрицами $\Phi^l_0$.  
	\item \textbf{Обучение иерархии:} используем модель hARTM для построения иерархической тематической модели коллекции $D$. На каждом уровне иерархии начальные значения параметров заданы на этапе инициализации.
\end{enumerate}


\section{Вычислительный эксперимент: агрегирование русскоязычного научно-популярного контента}
\todo{построить визуализации}
Опишем реализацию предложенного метода для задачи агрегирования русскоязычного научно-популярного контента на примере коллекций ПостНауки и Хабрахабра.

\subsection{Начальное приближение: иерархия ПостНауки} 

Первый этап --- построение модели ПостНауки, в которую в дальнейшем планируется добавлять документы Хабрахабра. 

 Модель содержит два уровня иерархии. На первом уровне 21 тема (одна фоновая). На втором уровне 61 тема (1 фоновая). Использовались модальности слов и тэгов, причем для тэгов был установлен значительно больший вес, что позволило существенно улучшить качество модели.

\textbf{Первый уровень иерархии}

Применялись регуляризаторы декорреляции терминов нефоновыx тем и сглаживания $\Phi$ для фоновой темы.

\begin{figure}[h]
    \centering 
    \includegraphics[width=1\textwidth]{img/pn_level0.png}
    \caption{\label{fig:pn_level0}Визуализация первого уровня иерархии ПостНауки}
\end{figure}

На рисунке \ref{fig:pn_level0} изображена визуализация полученной модели. В названия тем вынесены 3 наиболее вероятных для данной темы токена. 


\textbf{Второй уровень иерархии}
 
\begin{figure}[h]
    \centering 
    \includegraphics[width=1\textwidth]{img/pn_level1.png}
    \caption{\label{fig:pn_level1}Визуализация уровней иерархии ПостНауки. Первый и второй уровень содержат темы, на третьем уровне находятся отнесенные к темам документы.}
    \end{figure}

Применялась следующая стратегия регуляризации: 
\begin{itemize} \item первые 10 итераций применялись регуляризаторы декорреляции терминов нефоновых тем и сглаживания $\Phi$ фоновой темы;

\item после 10 итераций подмножества нефоновых тем, которые модель отнесла к одной родительской теме первого уровня, разреживались с помощью регуляризатора разреживания $\Theta$ между собой с коэффициентами $\tau$, возрастающими с каждой итерацией и пропорциональными количеству тем в каждом подмножестве.
\end{itemize}

На рисунке \ref{fig:pn_level1} изображены различные уровни иерархии в полученной модели, темы проименованные тремя наиболее вероятными токенами.

Полученная модель содержит существенно различные темы в силу того, что контент ПостНауки разнородный (содержит как естественнонаучные, так и гуманитарные и технические статьи). В дальнейшем будем предполагать, что в ней содержатся все характерные для научно-популярных статей темы. 

\subsection{Фильтрация коллекции Хабрахабра} 
Коллекция Хабрахабра была отранжирована по предложенной метрике близости. Затем полученные значения расстояний были отнормированы по метрике max. Было выбрано значение порогового расстояния $t = 0.85$. При этом в отфильтрованную коллекцию $D_f$ вошло 9288 документов, то есть размер добавленной коллекции в несколько раз превышает размер базовой коллекции. 


\subsection{Дополнение модели ПостНауки отфильтрованной коллекцией Хабрахабра}
Модель содержит два уровня иерархии. На первом уровне 21 тема (одна фоновая). На втором уровне 61 тема (1 фоновая).
Объединенная коллекция $D = D_0 \cup D_f$ содержит 12264 документа. 

\textbf{Первый уровень иерархии} 

$\Phi^1$ инициализировалась, как описано в алгоритме.
Применялись регуляризаторы декорреляции терминов нефоновыx тем с коэффициентом, увеличенным пропорционально увеличению размера коллекции и сглаживания $\Phi$ для фоновой темы.

На рис.4 изображена визуализация полученной модели. Видно, что полученная модель отличается от первого уровня иерархии
ПостНауки незначительно. Таким образом, добавленные документы из коллекции Хабрахабра распределились по существующим темам.

\begin{figure}[h]
    \centering 
    \includegraphics[width=1\textwidth]{img/pn_level0.png} \\
 \caption{Визуализация первого уровня иерархии}
    \end{figure}
    
\begin{table}[h]
\centering
\begin{tabular}[t]{p{22em}|p{2em}|p{2em}}
 Название темы $t$ & $|D_t|$ & $P_t$\\
\hline
\textit{'технологии', 'интернет', 'социальные сети'} & 6006& 0.95 \\
\hline
\textit{'математика', 'статистика', 'нейронные сети'} & 2434 & 0.93 \\
\hline
\textit{'экономика', 'сша', 'япония'} & 2203& 0.90  \\
\hline
\textit{'право', 'юриспруденция', 'закон'} & 1463&  0.92  \\
\hline
\textit{'образование', 'наука', 'университет'} & 1298& 0.86  \\
\hline
\textit{'философия', 'история философии', 'политическая философия'} & 142& 0.34  \\
\hline
\textit{'медицина', 'биомедицина', 'онкология'} &120& 0.28 \\
\hline
\textit{'биология', 'эволюция', 'днк'} & 111 & 0.15  \\
\hline
\textit{'история', 'история россии', 'средневековье'} & 94& 0.15  \\

\end{tabular}
\caption{\label{table:p_habr}Распределение новых документов по темам. $|D_t|$ -- количество документов Хабрахабра, отнесенных к теме $t$ по порогу вероятности $p(t|d) > 0.1$. $P_t$ -- доля документов Хабрахабра среди всех документов темы $t$}
\end{table}

\textbf{Второй уровень иерархии} 

$\Phi^2$ инициализировалась, как описано в алгоритме.

На втором уровне иерархии использовалась та же стратегия регуляризации, что и для второго уровня модели базовой коллекции. Коэффициент при декоррелирующем регуляризаторе был также изменен пропорционально размеру коллекции. 

В таблице \ref{table:p_habr} приведены темы построенной модели с соответствующими им количеством и долей статей Хабрахабра. Таблица содержит все темы с наибольшим количеством статей Хабрахабра и несколько примеров тем с малой долей статей Хабрахабра. Как и ожидалось, добавленные документы распределились по темам, характерным для Хабрахабра. При этом темы, характерные только для ПостНауки, содержат значительно меньше статей Хабрахабра. Так как объединенная коллекция содержит в основном документы Хабрахабра, такое распределение приводит к существенным различиям в размере тем, тогда как при базовом подходе к построению модели все темы имеют примерно одинаковый размер.

\section{Сравнение алгоритмов}

\begin{figure}[h]
    \centering 
    \includegraphics[width=1\textwidth]{img/alg_comparison.png}
    \caption{\label{fig:alg_comparison}Сравнение алгоритмов}
\end{figure}


