@article{Panchenko2015,
abstract = {Дармштадтский технический университет, Дармштадт, Германия Лувенский католический университет, Лувен, Бельгия Лукашевич Н. В. (louk{\_}nat@mail.ru) МГУ имени М. В. Ломоносова, Москва, Россия Усталов Д. (dau@imm.uran.ru) Институт математики и механики им. Н. Н. Красовского Уральского отделения Российской академии наук, Екатеринбург, Россия; NLPub, Екатеринбург, Россия Паперно Д. Мероприятие RUSSE, представленное на конференции «Диалог 2015», посвящено исследованию систем определения семантической бли-зости слов на русском языке. Для оценки таких систем предложено четыре подхода, основанных на человеческих оценках и классах се-мантических отношений. В мероприятии приняло участие 19 команд, приславших 105 моделей. Лучшие результаты показывают методы на основе обучения с учителем, сочетающие данные из разных источ-ников. Несмотря на это, методы без учителя, такие как дистрибутивные модели, обученные на большом корпусе, демонстрируют сравнимые результаты. В статье приведено описание мероприятия RUSSE и при-ведены результаты проведённого эксперимента на существительных русского языка. Ключевые слова: компьютерная лингвистика, лексическая семантика, меры семантической близости, семантические отношения, извлечение семантических отношений, синонимы, гиперонимы, когипонимы},
author = {Panchenko, A. and Loukachevitch, N. V. and Ustalov, D. and Paperno, D. and Meyer, C. M. and Konstantinova, N.},
issn = {20757182},
journal = {Компьютерная Лингвистика И Интеллектуальные Технологии: По Материалам Ежегодной Международной Конференции «Диалог»},
title = {{Russe: the First Workshop on Russian Semantic Similarity}},
year = {2015}
}



@article{Bouma2009,
  title={Normalized (pointwise) mutual information in collocation extraction},
  author={Bouma, Gerlof},
  journal={Proceedings of GSCL},
  pages={31--40},
  year={2009}
}

@inproceedings{Newman2010,
  title={Automatic evaluation of topic coherence},
  author={Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin, Timothy},
  booktitle={Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={100--108},
  year={2010},
  organization={Association for Computational Linguistics}
}

@inproceedings{Lau2014,
abstract = {Topic models based on latent Dirichlet allocation and related methods are used in a range of user-focused tasks including document navigation and trend analysis, but evaluation of the intrinsic quality of the topic model and topics remains an open research area. In this work, we explore the two tasks of automatic evaluation of single topics and automatic evaluation of whole topic models, and provide recommendations on the best strategy for performing the two tasks, in addition to providing an open-source toolkit for topic and topic model evaluation.},
address = {Stroudsburg, PA, USA},
archivePrefix = {arXiv},
arxivId = {1606.05908},
author = {Lau, Jey Han and Newman, David and Baldwin, Timothy},
booktitle = {Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics},
doi = {10.3115/v1/E14-1056},
eprint = {1606.05908},
isbn = {9781632663962},
issn = {9781632663962},
mendeley-groups = {ARTM},
pages = {530--539},
publisher = {Association for Computational Linguistics},
title = {{Machine Reading Tea Leaves: Automatically Evaluating Topic Coherence and Topic Model Quality}},
url = {http://aclweb.org/anthology/E14-1056},
year = {2014}
}



@article{Nikolenko2017,
abstract = {Nikolenko, Sergey I., Sergei Koltcov, and Olessia Koltsova. "Topic modelling for qualitative studies." Journal of Information Science 43.1 (2017): 88-102.},
author = {Nikolenko, Sergey I. and Koltcov, Sergei and Koltsova, Olessia},
doi = {10.1177/0165551515617393},
isbn = {0165-5515},
issn = {0165-5515},
journal = {Journal of Information Science},
keywords = {LDA extensions,Latent Dirichlet allocation,topic modelling,topic quality},
month = {feb},
number = {1},
pages = {88--102},
title = {{Topic modelling for qualitative studies}},
url = {http://journals.sagepub.com/doi/10.1177/0165551515617393},
volume = {43},
year = {2017}
}


@article{Chaney2012,
abstract = {Managing large collections of documents is an important$\backslash$nproblem for many areas of science, industry, and$\backslash$nculture. Probabilistic topic modeling offers a promising$\backslash$nsolution. Topic modeling is an unsupervised machine$\backslash$nlearning method that learns the underlying themes in$\backslash$na large collection of otherwise unorganized documents.$\backslash$nThis discovered structure summarizes and organizes the$\backslash$ndocuments. However, topic models are high-level statistical$\backslash$ntools—a user must scrutinize numerical distributions$\backslash$nto understand and explore their results. In this$\backslash$npaper, we present a method for visualizing topic models.$\backslash$nOur method creates a navigator of the documents,$\backslash$nallowing users to explore the hidden structure that a$\backslash$ntopic model discovers. These browsing interfaces reveal$\backslash$nmeaningful patterns in a collection, helping end-users$\backslash$nexplore and understand its contents in new ways. We$\backslash$nprovide open source software of our method.},
author = {Chaney, Ajb and Blei, Dm},
isbn = {9781577355564},
journal = {Icwsm},
mendeley-groups = {ARTM},
title = {{Visualizing Topic Models.}},
year = {2012}
}
@article{Blei2004,
abstract = {We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, gen- erating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior al- lows arbitrarily large branching factors and readily accommodates grow- ing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts.},
archivePrefix = {arXiv},
arxivId = {arXiv:0710.0845v2},
author = {Blei, D. and Griffiths, T.L. and Jordan, M.I. and Tenenbaum, J.B.},
doi = {10.1016/0169-023X(89)90004-9},
eprint = {arXiv:0710.0845v2},
isbn = {0262201526},
issn = {0169023X},
journal = {Advances in neural information processing systems},
mendeley-groups = {ARTM},
pmid = {25122015},
title = {{Hierarchical topic models and the nested Chinese restaurant process}},
year = {2004}
}
@inproceedings{Mimno2007,
abstract = {The four-level pachinko allocation model (PAM) (Li {\&} McCallum, 2006) represents correlations among topics using a DAG structure. It does not, however, represent a nested hierarchy of topics, with some topical word distributions representing the vocabulary that is shared among several more specific topics. This paper presents hierarchical PAM---an enhancement that explicitly represents a topic hierarchy. This model can be seen as combining the advantages of hLDA's topical hierarchy representation with PAM's ability to mix multiple leaves of the topic hierarchy. Experimental results show improvements in likelihood of held-out documents, as well as mutual information between automatically-discovered topics and humangenerated categories such as journals.},
author = {Mimno, David and Li, Wei and McCallum, Andrew},
booktitle = {Proceedings of the 24th international conference on Machine learning - ICML '07},
doi = {10.1145/1273496.1273576},
isbn = {9781595937933},
issn = {9781595937933},
mendeley-groups = {ARTM},
title = {{Mixtures of hierarchical topics with Pachinko allocation}},
year = {2007}
}
@inproceedings{Fang2016,
abstract = {Scholars often seek to understand topics discussed on Twitter using topic modelling approaches. Several coherence metrics have been proposed for evaluating the coherence of the topics generated by these approaches, including the pre-calculated Pointwise Mutual Information (PMI) of word pairs and the Latent Semantic Analysis (LSA) word representation vectors. As Twitter data contains abbreviations and a number of peculiarities (e.g. hashtags), it can be challenging to train effective PMI data or LSA word representation. Recently, Word Embedding (WE) has emerged as a particularly effective approach for capturing the similarity among words. Hence, in this paper, we propose new Word Embedding-based topic coherence metrics. To determine the usefulness of these new metrics, we compare them with the previous PMI/LSA-based metrics. We also conduct a large-scale crowdsourced user study to determine whether the new Word Embedding-based metrics better align with human preferences. Using two Twitter datasets, our results show that the WE-based metrics can capture the coherence of topics in tweets more robustly and efficiently than the PMI/LSA-based ones. {\textcopyright} 2016 ACM.},
author = {Fang, Anjie and Macdonald, Craig and Ounis, Iadh and Habel, Philip},
booktitle = {Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval - SIGIR '16},
doi = {10.1145/2911451.2914729},
isbn = {9781450340694},
mendeley-groups = {ARTM},
title = {{Using Word Embedding to Evaluate the Coherence of Topics from Twitter Data}},
year = {2016}
}
@inproceedings{Nikolenko2016,
abstract = {Automated evaluation of topic quality remains an impor-tant unsolved problem in topic modeling and represents a major obstacle for development and evaluation of new topic models. Previous attempts at the problem have been formu-lated as variations on the coherence and/or mutual informa-tion of top words in a topic. In this work, we propose several new metrics for evaluating topic quality with the help of dis-tributed word representations; our experiments suggest that the new metrics are a better match for human judgement, which is the gold standard in this case, than previously de-veloped approaches.},
author = {Nikolenko, Sergey I. and I., Sergey},
booktitle = {Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval - SIGIR '16},
doi = {10.1145/2911451.2914720},
isbn = {9781450340694},
keywords = {text mining,topic modeling,topic quality},
mendeley-groups = {ARTM},
title = {{Topic Quality Metrics Based on Distributed Word Representations}},
year = {2016}
}
@article{Schutze1993,
abstract = {This paper introduces context digests, high-dimensional real-valued representations for the typical left and right contexts of a word. Initial entries for the context digests are formed from the word's close left and right neighbors. A singular value decomposition reduces the dimensionality of the space to enable subsequent efficient processing. In contrast to similar techniques, no preprocessor such as a parser is required. Context digests summarize both syntagmatic and paradigmatic relations between words: how typical they are as neighbors and how well they are substitutable for each other. We apply context digests to identifying collocations, to assessing the similarity of the arguments of different verbs, and to clustering occurrences of adjectives and verbs according to the words they modify in context. 1 Introduction The electronic availability of large text corpora offers opportunities for both improving existing dictionaries and producing new dictionaries in a completely novel...},
author = {Sch{\"{u}}tze, Hinrich and Pedersen, Jan},
isbn = {9782760618688},
journal = {Making Sense of Words: Proceedings of the Conference},
mendeley-groups = {ARTM},
title = {{A Vector Model for Syntagmatic and Paradigmatic Relatedness}},
year = {1993}
}
@article{Chuang2012,
abstract = {Topic models aid analysis of text corpora by identifying la- tent topics based on co-occurring words. Real-world de- ployments of topic models, however, often require intensive expert verification and model refinement. In this paper we present Termite, a visual analysis tool for assessing topic model quality. Termite uses a tabular layout to promote comparison of terms both within and across latent topics. We contribute a novel saliency measure for selecting relevant terms and a seriation algorithm that both reveals clustering structure and promotes the legibility of related terms. In a series of examples, we demonstrate how Termite allows analysts to identify coherent and significant themes.},
author = {Chuang, Jason and Manning, Christopher D. and Heer, Jeffrey},
doi = {10.1145/2254556.2254572},
isbn = {9781450312875},
journal = {Proceedings of the International Working Conference on Advanced Visual Interfaces - AVI '12},
keywords = {seriation,text visualization,topic models},
mendeley-groups = {ARTM},
title = {{Termite : Visualization Techniques for Assessing Textual Topic Models}},
year = {2012}
}
@article{Blei2006,
abstract = {Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than x-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution. We derive a mean-field variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. The CTM gives a better fit than LDA on a collection of OCRed articles from the journal Science. Furthermore, the CTM provides a natural way of visualizing and exploring this and other unstructured data sets.},
archivePrefix = {arXiv},
arxivId = {arXiv:0712.1486v1},
author = {Blei, David M. and Lafferty, John D.},
doi = {10.1145/1143844.1143859},
eprint = {arXiv:0712.1486v1},
isbn = {1595933832},
issn = {19326157},
journal = {Advances in Neural Information Processing Systems 18},
mendeley-groups = {ARTM},
pmid = {9013932},
title = {{Correlated Topic Models}},
year = {2006}
}
@article{Marchionini2006,
abstract = {Research tools critical for exploratory search success involve the creation of new interfaces that move the process beyond predictable fact retrieval.},
archivePrefix = {arXiv},
arxivId = {test},
author = {Marchionini, Gary},
doi = {10.1145/1121949.1121979},
eprint = {test},
isbn = {0335220045},
issn = {00010782},
journal = {Communications of the ACM},
mendeley-groups = {ARTM},
pmid = {20371670},
title = {{Exploratory search: from finding to understanding}},
year = {2006}
}
@article{Hang2011,
abstract = {Learning to rank refers to machine learning techniques for training the model in a ranking task. Learning to rank is useful for many applications in Information Retrieval, Natural Language Processing, and Data Mining. Intensive stud- ies have been conducted on the problem and significant progress has been made [1], [2]. This short paper gives an introduction to learning to rank, and it specifically explains the fundamen- tal problems, existing approaches, and future work of learning to rank. Several learning to rank methods using SVM techniques are described in details.},
author = {Hang, Li},
doi = {10.1587/transinf.E94.D.1},
issn = {0916-8532},
journal = {IEICE Transactions on Information and Systems},
keywords = {information retrieval,language processing,learning to rank,natural,svm},
mendeley-groups = {ARTM},
title = {{A Short Introduction to Learning to Rank}},
year = {2011}
}
@inproceedings{Than2012,
abstract = {In this paper, we propose Fully Sparse Topic Model (FSTM) for modeling large collections of documents. Three key properties of the model are: (1) the inference algorithm converges in linear time, (2) learning of topics is simply a multiplication of two sparse matrices, (3) it provides a principled way to directly trade off sparsity of solutions against inference quality and running time. These properties enable us to speedily learn sparse topics and to infer sparse latent representations of documents, and help significantly save memory for storage. We show that inference in FSTM is actually MAP inference with an implicit prior. Extensive experiments show that FSTM can perform substantially better than various existing topic models by different performance measures. Finally, our parallel implementation can handily learn thousands of topics from large corpora with millions of terms.},
author = {Than, Khoat and Ho, Tu Bao},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-33460-3_37},
isbn = {9783642334597},
issn = {03029743},
mendeley-groups = {ARTM},
title = {{Fully sparse topic models}},
year = {2012}
}
@article{Chemudugunta2006,
abstract = {Techniques such as probabilistic topic models and latent-semantic indexing have been shown to be broadly useful at automatically extracting the topical or semantic content of documents, or more generally for dimension-reduction of sparse count data. These types of models and algorithms can be viewed as generating an abstraction from the words in a document to a lower-dimensional latent variable representation that captures what the document is generally about beyond the specific words it contains. In this paper we propose a new probabilistic model that tempers this approach by representing each document as a combination of (a) a background distribution over common words, (b) a mixture distribution over general topics, and (c) a distribution over words that are treated as being specific to that document. We illustrate how this model can be used for information retrieval by matching documents both at a general topic level and at a specific word level, providing an advantage over techniques that only match documents at a general level (such as topic models or latent-sematic indexing) or that only match documents at the specific word level (such as TF-IDF).},
author = {Chemudugunta, C and Smyth, P and Steyvers, M},
isbn = {0262195682},
issn = {10495258},
journal = {Nips},
mendeley-groups = {ARTM},
title = {{Modeling general and specific aspects of documents with a probabilistic topic model}},
year = {2006}
}
@article{Rosen-Zvi2004,
abstract = {We introduce the author-topic model, a gen- erative model for documents that extends La- tent Dirichlet Allocation (LDA; Blei, Ng, {\&} Jordan, 2003) to include authorship informa- tion. Each author is associated with a multi- nomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple au- thors is modeled as a distribution over topics that is a mixture of the distributions associ- ated with the authors. We apply the model to a collection of 1,700 NIPS conference pa- pers and 160,000 CiteSeer abstracts. Exact inference is intractable for these datasets and we use Gibbs sampling to estimate the topic and author distributions. We compare the performance with two other generative mod- els for documents, which are special cases of the author-topic model: LDA (a topic model) and a simple author model in which each au- thor is associated with a distribution over words rather than a distribution over top- ics. We show topics recovered by the author- topic model, and demonstrate applications to computing similarity between authors and entropy of author output.},
archivePrefix = {arXiv},
arxivId = {1207.4169},
author = {Rosen-Zvi, M. and Griffiths, T. and Steyvers, M. and Smyth, P.},
doi = {10.1016/j.nima.2010.11.062},
eprint = {1207.4169},
isbn = {0-9749039-0-6},
issn = {01689002},
journal = {Proceedings of the 20th conference on Uncertainty in artificial intelligence},
mendeley-groups = {ARTM},
title = {{The author-topic model for authors and documents}},
year = {2004}
}
@inproceedings{Mimno2011,
abstract = {Latent variable models have the potential to add value to large document collections by discovering interpretable, low-dimensional subspaces. In order for people to use such models, however, they must trust them. Un-fortunately, typical dimensionality reduction methods for text, such as latent Dirichlet allocation, often produce low-dimensional sub- spaces (topics) that are obviously flawed to human domain experts. The contributions of this paper are threefold: (1) An analysis of the ways in which topics can be flawed; (2) an automated evaluation metric for identifying such topics that does not rely on human annotators or reference collections outside the training data; (3) a novel statistical topic model based on this metric that significantly improves topic quality in a large-scale document collection from the National Institutes of Health (NIH).},
author = {Mimno, David and Wallach, Hanna M. and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
booktitle = {Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing},
isbn = {9781937284114},
issn = {1937284115},
keywords = {topic coherence,topic models,topics evaluation},
mendeley-groups = {ARTM},
title = {{Optimizing semantic coherence in topic models}},
year = {2011}
}
@article{Fan2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1701.03227v2},
author = {Fan, Angela and Doshi-velez, Finale and Miratrix, Luke},
eprint = {arXiv:1701.03227v2},
file = {:Users/maria.selezniova/lib/Papers/Fan, Doshi-velez, Miratrix/Prior matters simple and general methods for evaluating and improving topic quality in topic modeling/Fan, Doshi-velez, Miratrix - 2017 - Prior matters simple and general methods for evaluating and improving topic quality in topic modeli.pdf:pdf},
mendeley-groups = {ARTM},
pages = {1--16},
title = {{Prior matters : simple and general methods for evaluating and improving topic quality in topic modeling}},
year = {2017}
}
@article{B2017,
author = {B, Bo Wang and Liakata, Maria and Zubiaga, Arkaitz and Procter, Rob},
doi = {10.1007/978-3-319-67256-4},
file = {:Users/maria.selezniova/lib/Papers/B et al/for Tweet Clustering/B et al. - 2017 - for Tweet Clustering.pdf:pdf},
isbn = {9783319672564},
keywords = {tweet clustering},
mendeley-groups = {ARTM},
pages = {378--390},
title = {{for Tweet Clustering}},
volume = {1},
year = {2017}
}
@article{Fang,
author = {Fang, Anjie and Macdonald, Craig and Ounis, Iadh and Habel, Philip},
file = {:Users/maria.selezniova/lib/Papers/Fang et al/Using Word Embedding to Evaluate the Coherence of Topics from Twitter Data/Fang et al. - Unknown - Using Word Embedding to Evaluate the Coherence of Topics from Twitter Data.pdf:pdf},
isbn = {9781450340694},
mendeley-groups = {ARTM},
pages = {1057--1060},
title = {{Using Word Embedding to Evaluate the Coherence of Topics from Twitter Data}}
}
@article{..,
author = {К.В., Воронцов},
file = {:Users/maria.selezniova/lib/Papers/К.В/Тематическое моделирование АРТМ, обхор 2017/К.В. - Unknown - Тематическое моделирование АРТМ, обхор 2017.pdf:pdf},
mendeley-groups = {ARTM},
title = {{Тематическое моделирование АРТМ, обхор 2017}}
}
@misc{Nikolenko,
author = {Nikolenko},
file = {:Users/maria.selezniova/lib/Papers/Nikolenko/Topic Quality Metrics Based on DistributedWord Representations/Nikolenko - Unknown - Topic Quality Metrics Based on DistributedWord Representations.pdf:pdf},
mendeley-groups = {ARTM},
title = {{Topic Quality Metrics Based on DistributedWord Representations}}
}
@article{Jmlda20162016,
author = {Jmlda2016},
file = {:Users/maria.selezniova/lib/Papers/Jmlda2016/Машинное обучение и анализ данных/Jmlda2016 - 2016 - Машинное обучение и анализ данных.pdf:pdf},
journal = {JMLDA},
mendeley-groups = {ARTM},
title = {{Машинное обучение и анализ данных}},
year = {2016}
}
@article{Liu2016,
author = {Liu, Lin and Tang, Lin and He, Libo and Zhou, Wei and Yao, Shaowen},
doi = {10.1109/IHMSC.2016.101},
file = {:Users/maria.selezniova/lib/Papers/Liu et al/An overview of Hierarchical topic modeling/Liu et al. - 2016 - An overview of Hierarchical topic modeling.pdf:pdf},
isbn = {9781509007684},
keywords = {-topic modeling,generative,hierarchical topic,natural language processing,process},
mendeley-groups = {ARTM},
title = {{An overview of Hierarchical topic modeling}},
year = {2016}
}
@article{Lau2014,
author = {Lau, Jey Han and Newman, David and Baldwin, Timothy},
file = {::},
mendeley-groups = {ARTM},
pages = {530--539},
title = {{Machine Reading Tea Leaves : Automatically Evaluating Topic Coherence and Topic Model Quality}},
year = {2014}
}
